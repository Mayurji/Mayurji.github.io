---
layout: post
title:  Iterators and Generators in Python
description: Saving memory is an essential aspect of performant programming
category: Blog
date:   2020-12-05 13:43:52 +0530
---

In this post, We'll discuss the python's ***Iterators and Generators*** objects and decode why generators are memory efficient and why iterators are used over generators irrespective of memory usage.

<center>
<img src="{{site.url}}/assets/images/dicts_sets/front-ds.png" class="post-body" style="zoom: 5%; background-color:#DCDCDC;" width="1000" height="600"/><br>
<p>Figure 1: Data Structures</p>
</center>

### Iterators

In python, *for* loop requries an object we are looping through to support iterator function, python provides a built-in function *__iter__* to convert lists, set, dictionary, tuples into an iterator such that we can iterate over keys or items in that object. While building an iterator like list iterator, a number of functions are built like *__iter__*, *__next__*, keeping track of the states and raising exception *StopIteration* when no values are present to iterate over.

### Dismantling For loops

```python
# The Python loop
for i in object:
    do_work(i)

# Is equivalent to
object_iterator = iter(object)
while True:
    try:
        i = next(object_iterator)
    except StopIteration:
        break
    else:
        do_work(i)
```

<center>
<img src="{{site.url}}/assets/images/iterators/iterators.png" class="post-body" style="zoom: 5%; background-color:#DCDCDC;" width="1000" height="800"/><br>
<p>Figure 1: Iterators</p>
</center>

*List iterator based fibonacci series*

```python
def fibonacci_list(num_items):
    numbers = []
    a, b = 0, 1
    while len(numbers) < num_items:
        numbers.append(a)
        a, b = b, a+b
    return numbers
```
### Generators

Generator is a function that returns an object(iterator) which we can iterate over (one value at a time). A normal function terminates with return statement, it evaluation comes out of the function, while a generator function *yields* the results of evaluation while pausing the execution, starts again with next value of iteration, during the pause state, the control is passed to the caller. Function like *__iter__*, *__next__* are implemented automatically. 

*Generator based fibonacci series*

```python
def fibonacci_gen(num_items):
    a, b = 0, 1
    while num_items:
    	yield a  
        a, b = b, a+b
        num_items -= 1
```

In Generator, we don't store the element in array for further evaluation/usage, unlike in lists, where we can reference the list anywhere, without performing iteration over all elements again.

<center>
<img src="{{site.url}}/assets/images/iterators/generator.png" class="post-body" style="zoom: 5%; background-color:#DCDCDC;" width="1000" height="800"/><br>
<p>Figure 3: Generator</p>
</center>

### Memory Efficiency 

A major benefit of using a Generator is the memory saved during the iteration, because don't store the element anywhere. For instance, consider an iteration over a million numbers, if we store the numbers in a list, it occupies hundreds of megabytes for storing it, while on the other hand, generator there is no concept for storing the items, we perform lazy evaluation when the generator is called.

```python
def test_fibonacci_list():
    """
    >>> %timeit test_fibonacci_list()
    332 ms ± 13.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

    >>> %memit test_fibonacci_list()
    peak memory: 492.82 MiB, increment: 441.75 MiB
    """
    for i in fibonacci_list(100_000):
        pass


def test_fibonacci_gen():
    """
    >>> %timeit test_fibonacci_gen()
    126 ms ± 905 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

    >>> %memit test_fibonacci_gen()
    peak memory: 51.13 MiB, increment: 0.00 MiB
    """
    for i in fibonacci_gen(100_000):
        pass
```

Right now, we may believe we should utilize generators everywhere to save the memory, but what if the results generated by iterator (either be list or generator) are to be referenced in future. For referencing in future, we should iterate over generator again to get the processed results again, but instead we can utilize all the processed elements stored in a list. 

Thus, it comes to down what you want optimize, ***memory consumption or CPU optimization***. Extra memory helps in storing the processed result and if the memory is a constrain, then recalculate the processed element again using generator.

*It is important to note that many of python's built-in function like range, map, zip, filter, reveresed or enumerate utilize generator in the backend, they all perform the calculations required and doesn't store the result.*

### Generator Example

**Analysis**

*Consider working on a NLP based project, where we iterate over millions of text samples from csv file or text file for text processing or clean up. If we use a generator, we might end up doing processing without storing the processed text. And performing nlp text processing is computionally expensive task, it better to avoid generator on such occasion.*

*Anyway, while working on such huge files, we can apply filter function on each row of text samples, the filter function, internally runs an generator based iteration, just fetching the records with conditon (mentioned below example, finding numbers divisible by 3).*

**Reduce memory consumption**

*Creating a list of numbers that are divisible by 3. The memory consumption is high and can be avoided by using generator.*

*List Iterator*

```python
divisible_by_three = len([n for n in fibonacci_gen(100000) if n % 3 == 0])
```

*Generator Function*

```python
divisible_by_three = sum(1 for n in fibonacci_gen(100000) if n % 3 == 0)
```

In conclusion, both the iterators and generator are applied based on requirement, as mentioned earlier the trade-off between the CPU optimization and Memory consumption. It's upto the individual, how to utilize these python objects efficiently.

### Reference

* [High Performance Python](https://www.oreilly.com/library/view/high-performance-python/9781449361747/)
